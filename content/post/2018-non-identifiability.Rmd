---
title: "Identifying non-identifiability"
date: 2018-05-14
tags: ["R","Stan","Divergences"]
---

*This is a second post in my series on taming divergences in Stan models, see the [first post in the series](/2018/02/19/taming-divergences-in-stan-models/) for a general introduction. *

**Standard caveat:** *I am not an expert on Stan, I consider myself just an advanced user who likes to explain things. Please point out any errors, things that contradict your experience or anything else you do not trust.*

# What is "non-identifiability"

In a strict sense, it means that two values of the parameters result in the same probability distribution of observed data. It is also sometimes used to cover situations when there is not a unique local maximum of the posterior density - either because there are multiple separate maxima or because there is ridge/plateau where a set of points has the same posterior density (those may or may not be identifiable in the strict sense).

On the Stan forums the term seems to be used in even a bit broader sense and also covers cases where the maximum of the posterior density is in a region that is approximately flat. This often happens when the posterior is dominated by the prior and the likelihood provides little information about the parameters. If this is the case, it is sometimes said that the model is *weakly identified*. A weakly identified model may become non-identified in the strict sense if a prior is not specified for all parameters. This is just another reason to specify proper priors for everything.

Problems with identifiability are just one class of issues that are signalled in Stan by divergences and/or other diagnostics (max treedepth, low BFMI, low `n_eff`, large `Rhat`), the [first post in this series](/2018/02/19/taming-divergences-in-stan-models/) has a more extensive list of other possible causes. Note also that except for Stan, most statistical/ML software won't complain when you try to fit non-identifiable models, even though it may lead to noticeably biased inferences.

# Scope

In this post I will show a few different types of issues that result from limited identifiability. I will also try to show how to spot these problems in various visualisations. Remember that instead of creating the plots in code as we do here, you can use [ShinyStan](http://mc-stan.org/users/interfaces/shinystan) to explore many visualisations interactively. 

We will start with some weakly identified non-linear regression models and move toward models that are hopelessly multimodal (have multiple local maxima of posterior density), including a small neural network and Gaussian process with a Berkson-style error. I will focus on models that don't have any obvious error (like ommitted prior), although such errors can lead to non-identifiability. A recurring theme in this post is that identifiability may depend on the actual data as well as the model, keep that in mind when modelling!

A frequent source of fitting issues due to non-identifiability are mixture models. There is an excellent
[case study for mixture models](http://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html) by Michael Betancourt and I have nothing to add to this topic, so mixtures are not covered here.

The post got pretty long so let's not hesitate and get our hands dirty!

```{r setup, results='hide',message=FALSE,warning=FALSE}
library(tidyverse)
library(rstan)
library(bayesplot)
library(tidybayes)
library(knitr)
library(here)
library(rgl)
knit_hooks$set(webgl = hook_webgl)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```


# A weakly-identified linear model that (mostly) works

First, let's start with a model I thought would have problems but that ended up mostly OK. The model is a simple regression with a quadratic term:

$$
\begin{align}
\mu_i &= \beta_1 x_i + \beta_2 x_i^2 \\
y_i &\sim N(\mu_i, \sigma)
\end{align}
$$
and here is the Stan code:

```{r cache=TRUE, results='hide'} 
stan_code_linear <- " 
data { 
  int N; 
  vector[N] y; 
  vector[N] x; 
  real<lower=0> sigma; 
  real<lower=0> prior_width;
} 
 
parameters { 
  real beta[2]; 
} 
 
model { 
  y ~ normal(beta[1] * x + beta[2] * square(x), sigma); 
  sigma ~ normal(0,1); 
  beta ~ normal(0,prior_width); 
} 
" 
 
model_linear <- stan_model(model_code = stan_code_linear) 
``` 

When wide array of $x$ values is available, this model works without any trouble. But what happens when all $x_i \in  \{0,1\}$? In this case the likelihood cannot distinguish between the contribution of $\beta_1$ and $\beta_2$. Let's simulate some data and have a look at the [pairs](https://www.rdocumentation.org/packages/rstan/versions/2.17.3/topics/pairs.stanfit) plot:

```{r cache=TRUE} 
set.seed(20180512)

sigma = 1 
x = rep(c(0,1), times = 10)
data_linear <- list( 
  N = length(x), 
  x = x, 
  y = rnorm(length(x), x + x ^ 2, sigma),
  sigma = sigma,
  prior_width = 10
) 
 
fit_linear <- sampling(model_linear, data = data_linear) 
pairs(fit_linear, pars = "beta")
``` 

While the result is not very useful, the sampler worked well! We gained little information about each $\beta$ individually (their range spans alomost all of the prior), but their sum is tightly constrained as witnessed by the strong negative correlation. So what if we increase `prior_width` to make the prior resemble a flat prior? We do get max treedepth warnings!

```{r cache=TRUE}
set.seed(21645465)

data_linear2 <- data_linear
data_linear2$prior_width = 100
fit_linear2 <- sampling(model_linear, data = data_linear2) 
pairs(fit_linear2, pars = "beta")
```

What seems to happen is that the ridge in the posterior becomes too long and the sampler cannot traverse it efficiently, resulting in transitions that exceed the maximum treedepth. The root of the problem is that the sampler has to choose a step size that is shorter than the width of the ridge to not diverge when moving tangentially to the ridge direction. With such a small step size, the sampler cannot move across the length of the ridge in one iteration. Still, the results should be unbiased and if we manage to get a reasonable `n_eff` (which we do), there is no reason to worry. Improper flat prior would however lead to some actual trouble. Don't use improper priors, folks!

**Take home message**: While this particular model works well unless we set the prior too wide, linear correlations in the pairs plot are a bad sign and you should try to avoid them as they can produce problems when interacting with other components of a larger model. It might make sense to reparametrize using the sum or ratio of the variables in question.

# A weakly-identified sigmoid model

Let's move to a model where the non-identifiability actually wreaks havoc. Once again, the model works for some data, but breaks for others - even if all model assumptions do hold. The model tries to determine parameters of a sigmoid function that is observed noisily:

$$
\begin{align}
y_{true} &= \frac{1}{1 + e^{-wx-b}} \\
y &\sim N(y_{true},\sigma)
\end{align}
$$

Here $w$ and $b$ are the only parameters of the model. The model is a bit artificial, but it is actually a component of larger gene expression models I work with. The corresponding Stan code is below:

```{r cache=TRUE, results = 'hide'}
stan_code_sigmoid <- "
data {
  int N;
  vector[N] y;
  vector[N] x;
  real<lower=0> prior_width;
  real<lower=0> sigma;
}

parameters {
  real w;
  real b;
}

model {
  vector[N] y_true = inv_logit(w * x + b);
  y ~ normal(y_true, sigma);
  w ~ normal(0,prior_width);
  b ~ normal(0,prior_width);
}
"

model_sigmoid <- stan_model(model_code = stan_code_sigmoid)
```

Now lets fit the model to simulated datasets with the exact same true parameter values $w = b = 1$, but different values of the independent variable $x$. In the first case, $x$ is drawn from $N(0,2)$:

```{r cache=TRUE}
set.seed(214575878)

simulate_sigmoid <- function(x) {
  sigma = 0.1
  w = 1
  b = 1
  N = length(x)
  y_true = 1 / (1 + exp(-w*x-b))
  prior_width = 10
  
  list(
    N = N,
    x = x,
    y = rnorm(N, y_true, sigma),
    prior_width = prior_width,
    sigma = sigma
  )
}

data_sigmoid_ok <- simulate_sigmoid(rnorm(20, 0, 2))
fit_sigmoid_ok <- sampling(model_sigmoid, data = data_sigmoid_ok)
print(fit_sigmoid_ok)
pairs(fit_sigmoid_ok, pars = c("w","b"))
```

For the first dataset, the model converges and recovers parameters correctly. The pairs plot shows a nice gaussian blob, nothing to worry about. Now let's try a second dataset, this time $x$ is drawn from $N(5,2)$.

```{r cache=TRUE}
data_sigmoid_divergent <- simulate_sigmoid(rnorm(20, 5, 2)) 
fit_sigmoid_divergent <- sampling(model_sigmoid, data = data_sigmoid_divergent)
print(fit_sigmoid_divergent)
```

For the second dataset, there is a huge number of divergences and the parameters are largely uncertain and overestimated. So once again, the model is identifiable in principle, but the extreme data introduce problems. 

Let's try to visualise the posteriors - luckily we only have two parameters, so it is easy to see everything at once!

```{r}
pairs(fit_sigmoid_divergent, pars = c("w","b"))
```

The issue seems to be that for the second dataset, we sampled $x$ towards the tail of the sigmoid and almost all $y$ are thus close to 1, giving us little information about the parameters. However, the model strictly enforces that $w x + b > 0$.
This creates a large area of the posterior where the value of $w$ and $b$ does not matter much (requiring a large step size to traverse) and a thin sharp boundary around $wx + b \simeq 0$ where a smaller step size is required to traverse safely. Transitions crossing the boundary often diverge due to large step size and are rejected, leading to overexploration of the flat area and bias.

This is, in my experience, one of the ways that weak identifiability may hurt sampling - the model maybe weakly identified only for a subset of the parameter space while there is another area where the likelihood has a huge contribution and this may require different step size for the sampler. 

The issue may be partially redeemed by reparametrization using $a = \mathrm{E}(wx + b), b = \mathrm{sd}(wx + b)$. You can then set priors on $a, b$ that avoid the tails of the sigmoid while being independent of $x$.

**Take home message**: Sharp boundaries of otherwise diffuse regions in the posterior (as seen above) are  worth investigating.


# A sigmoid model with non-identified special case

We can make the above model even more problematic by introducing a parameter $k$ to generalize the sigmoid a bit more:

$$
\begin{align}
y_{true} &= \frac{k}{1 + e^{-wx-b}} \\
y &\sim N(y_{true},\sigma)
\end{align}
$$

giving us the following Stan code:

```{r cache=TRUE, results ='hide'}
stan_code_sigmoid2 <- "
data {
  int N;
  vector[N] y;
  vector[N] x;
  real<lower=0> prior_width;
  real<lower=0> sigma;
}

parameters {
  real k;
  real w;
  real b;
}

model {
  vector[N] y_true = k * inv_logit(w * x + b);
  y ~ normal(y_true, sigma);
  w ~ normal(0,prior_width);
  b ~ normal(0,prior_width);
}
"

model_sigmoid2 <- stan_model(model_code = stan_code_sigmoid2)
```

This time we will simulate two datasets with the same $x \sim N(0,2)$, avoiding the extreme values. We will set $k = b = 1$ for both datasets. In addition, the first dataset  will have $w = 1$. Let's see how that goes:

```{r cache=TRUE}
set.seed(98321456)

simulate_sigmoid2 <- function(x, w) {
  sigma = 0.1
  k = 1
  b = 1
  N = length(x)
  prior_width = 10
  y_true = k / (1 + exp(-w*x-b))
  
  list(
    N = N,
    x = x,
    y = rnorm(N, y_true, sigma),
    prior_width = prior_width,
    sigma = sigma
  )
}

data_sigmoid2_ok <- simulate_sigmoid2(rnorm(20, 0, 2), w = 1)
fit_sigmoid2_ok <- sampling(model_sigmoid2, data = data_sigmoid2_ok)
print(fit_sigmoid2_ok)
```

Nice, no problems here. Let's see what happens when we set $w = 0$

```{r cache=TRUE}
data_sigmoid2_divergent <-  simulate_sigmoid2(rnorm(20, 0, 2), w = 0)
fit_sigmoid2_divergent <- sampling(model_sigmoid2, data = data_sigmoid2_divergent)
print(fit_sigmoid2_divergent)
```


As has become the habit in this post, the model diverges for the second dataset. Note also that the parameter estimates for $k$ and $b$ are way off and their 95% credible intervals exclude the true value! (remember: even a few divergent transitions indicate sampling problems). Let's have a look at the pair plots for both models:

```{r}
pairs(fit_sigmoid2_ok, pars = "energy__", include = FALSE)
pairs(fit_sigmoid2_divergent, pars = "energy__", include = FALSE)
```

Clearly something substantial changed for the second model. But while the interaction of $w$ and $b$ looks fishy, it is hard to understand what exactly is going on. The clue is in looking at the interaction of $w$, $k$ and the log posterior (`lp__`) - you can play with the 3D plot below with your mouse. Note that ShinyStan provides similar 3D plots under `Explore -> Trivariate`.

```{r, webgl=TRUE, message = FALSE}
samples <- rstan::extract(fit_sigmoid2_divergent)
open3d() %>% invisible()
rgl::plot3d(samples$w, samples$k, samples$lp__, xlab = "w", ylab = "k", zlab = "lp__")
```

The thing to notice here is that for $w = 0$, there is a thin ridge in the posterior with almost maximal `lp__` for a wide range of values for $k$. This is because for $w = 0$, the posterior ceases to depend on $x$ and $k$ becomes very closely tied to $b$ - simultaneously almost any value of $k$ that is admitted by the prior becomes feasible with suitable value of $b$. The ridge is so thin, that the sampler almost completely misses it and it is barely visible in the plot. But for the true posterior, this ridge should contribute a non-trivial amount of mass. Once again, the sampler adapts its step size to the wide distribution for $w \neq 0$ which leads to divergences and rejections when traversing the narrow ridge at $w = 0$, bringing the problem to our attention.

What can you do about these kinds of problems? When you fit your model interactively, the best solution is to spot that your actual data are a special case and simplify your model. Special cases however become more of a worry when you need to automatically fit the model to a large number of datasets or refit it periodically as new data become available. The best solution I have found so far is to try fitting a simplified model first (here something like $x \sim N(a,\sigma)$). If the simplified model fits well and/or the full model diverges, the results of the simplified model are preferred. If you know of a better solution, please let me know - it is directly relevant to my work on [models of gene regulation](/post/2018-04-18-stancon/stancon.html)! The link also contains more discussion of the reparametrizations I use to make this kind of models converge.

**Take home message**: Worry about degenerate special cases of your model. A 3D trivariate plot of two parameters vs. the posterior, makes it really neatly visible when your posterior is not unimodal.


# Neural network: When ordering is not enough

At this point it would make sense to mention mixture models, but as those are covered by the aforementioned [case study](http://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html), we'll go straight ahead to the desperate wilderness of models too broken to fix. And neural networks are a prime attraction in this godforsaken land.

We don't need to go fancy. Let's have a feedforward neural net with two inputs, two hidden layers of two neurons each and a single output neuron.

![](/post/2018-non-identifiability_files/nn.png)

We will use the standard logistic sigmoid activation function and treat the problem as a binary classification. To make things simpler and because we saw that sigmoid may be non-dentifiable by itself, we ignore all the bias parameters, so the only parameters are the weights $w$ of inputs $x$ and the activation function becomes:

$$
\frac{1}{1+e^{-\sum w_i x_i}}
$$

Below is the corresponding Stan model - optimized for readability, not brevity or generalizability. Since it seems there might be some symmetries, and we learned our lesson from mixture models, we'll try at least to force the weights for the output neuron to be ordered.

```{r, results='hide', cache=TRUE}
stan_code_neural <- "


data {
  int N;
  matrix[N,2] x;
  int<lower=0, upper=1> y[N];
  real prior_width;
}

parameters {
  matrix[2,2] weights1;
  matrix[2,2] weights2;
  ordered[2] weights_out;
}

model {
  matrix[N,2] input1 = x * weights1;
  matrix[N,2] output1 = inv_logit(input1);

  matrix[N,2] input2 = output1 * weights2;
  matrix[N,2] output2 = inv_logit(input2);

  vector[N] input_out = output2 * weights_out;
  vector[N] output_out = inv_logit(input_out);
  y ~ bernoulli(output_out);

  to_vector(weights1) ~ normal(0, prior_width);
  to_vector(weights2) ~ normal(0, prior_width);
  weights_out ~ normal(0, prior_width);
}
" 

model_neural <- stan_model(model_code = stan_code_neural)
```

In the spirit of the best traditions of the field of machine learning, we'll try to teach XOR to the neural network. It does not go well. To make the pathologies better visible, we will use 8 chains instead of the usual 4.

```{r, cache=TRUE}
set.seed(1324578)
sigma <- 0.1
N <- 200

x <- array(as.integer(rbernoulli(N * 2)), c(N,2))
y <- xor(x[,1], x[,2])

data_neural <- list(N = N, x = x, y = y, sigma = sigma, prior_width = 5)
fit_neural <- sampling(model_neural, data = data_neural, chains = 8)

show_param_diags <- function(fit) {
  summary(fit)$summary[,c("n_eff","Rhat")]
}
show_param_diags(fit_neural)
```

We note the divergences, very low `n_eff` and large `Rhat`. Remember that `n_eff` (effective sample size) is a measure of how well the posterior was sampled and `Rhat` is close to one if the chains have converged to the same posterior. This time let's start by inspecting some traceplots - I chose the ones I consider most interesting but in practice you would want to look at all of them (once again those are also available in ShinyStan):

```{r, warning = FALSE, message = FALSE, fig.height=5}
fit_for_bayesplot <- as.array(fit_neural)
mcmc_trace(fit_for_bayesplot, pars = c("weights2[1,2]","weights1[2,2]"), 
           facet_args = list(ncol = 1)) + scale_color_discrete()
```

We clearly see that there are multiple modes and each chain is stuck in its mode and does not mix with others. The first trace plot shows that just investigating the marginal posterior for `weights2[1,2]` reveals 3 well separated modes. Looking at the traceplot for `weights1[2,2]` we see that there have to be even more modes as here, chain 2 (green-brown-ish?) clusters with chain 5 (blue), while in the first traceplot it clusters with chain 8 (pink). 

Looking at the pairs plot provides some additional hints:

```{r}
pairs(fit_neural, pars = c("weights2[1,2]","lp__"))
```

We see that at least the two "denser" modes are symmetric across zero and that they reach about the same maximum `lp__` as the "diffuse" mode. This means that to identify the model we need have to somehow choose one of those modes, but there is clearly not a "best" (much higher `lp__`) mode.

My best guess is that the divergences arise when the sampler (unsuccesfully) tries to switch between individual modes and the geometry gets narrower in some parameters and wider in others, but this is just guesswork. 


There is not really much that can be done to make such models work. The most obvious issues come from symmetries of the network structure, providing multiple modes when the neurons are relabelled, but the network is isomorphic. To some extent we could get rid of them by ordering one of the weights in each layer. However, ordering is just the start and further issues just keep on coming - see for example [the forum thread on Bayesian neural networks](http://discourse.mc-stan.org/t/why-are-bayesian-neural-networks-multi-modal/3285/3) for more details.

**Take home message**: Non overlaping traces without treedepth warnings indicate multimodality. If the modes have about the same `lp__`, some symmetry breaking constraints may help. If there is one mode with much larger `lp__` than the others, it might make sense to favor this one by appropriate priors/reparametrization.


# A hopelessly non-identified GP model

In the above example, there were multiple discrete and well separated modes. But there is still a way to move non-identifiability to the next level. We'll start with a simple and harmless [Gaussian process](https://en.wikipedia.org/wiki/Gaussian_process) model with squared exponential covariance function, the model is:

$$
\begin{align}
y_{est} &\sim GP(\rho,\tau) \\
y &\sim~ N(y_{est}(x), \sigma)
\end{align}
$$

Here, $x \in (0,1)$ are the locations where the GP is observed.
The corresponding Stan code is:

```{r, results='hide', cache=TRUE}
stan_code_gp <- "
data {
  int N;
  real x[N];
  vector[N] y;
  real<lower=0> gp_length;
  real<lower=0> gp_scale;
  real<lower=0> sigma;
}

transformed data {
  vector[N] mu = rep_vector(0, N);
}

parameters {
  vector[N] y_est_raw;
}

transformed parameters {
  vector[N] y_est;
  // Using the latent variable GP coding form Stan manual, 
  // with the Cholesky decomposition
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, gp_scale, gp_length);
    for (n in 1:N) {
      K[n, n] = K[n, n] + 1e-12; //Ensure positive definite
    }
    L_K = cholesky_decompose(K);
    y_est = L_K * y_est_raw;
  }
}

model {
  y ~ normal(y_est, sigma);
  y_est_raw ~ normal(0, 1);
}

"

model_gp <- stan_model(model_code = stan_code_gp)
```

We simulate some data and check that the model works well:

```{r, cache=TRUE}
set.seed(25748422)
simulate_gp <- function(x) {
  N <- length(x)
  gp_length <- 0.3
  gp_scale <- 1
  sigma <- 0.1
  cov_m <- matrix(0, nrow <- N, ncol <- N)
  for(i in 1:N) {
    for(j in i:N) {
      cov_m[i,j] <- gp_scale ^ 2 * exp(-0.5 * (1 / gp_length ^ 2) * (x[i] - x[j])^2)
      cov_m[j,i] <- cov_m[i,j]
    }
  }
  chol_cov_m <- chol(cov_m)
  y <- chol_cov_m %*% rnorm(N, 0, 1)
  
  list(N = N, x = x, y = array(y, N), gp_length = gp_length, gp_scale = gp_scale, sigma = sigma)
}

data_gp <- simulate_gp(x = seq(0.01,0.99, length.out = 10))
fit_gp <- sampling(model_gp, data = data_gp)
show_param_diags(fit_gp) %>% head()
```

We note the good diagnostics and also look at the posterior draws versus the observed values.

```{r, warning = FALSE}
samples_to_show <- sample(1:4000, 50)
fit_gp %>% 
  tidybayes::spread_samples(y_est[x_index]) %>% 
  inner_join(data.frame(x_index = 1:data_gp$N, x = data_gp$x), by = c("x_index" = "x_index")) %>% 
  mutate(sample_id = (.chain - 1) * 1000 + .iteration ) %>%
  filter(sample_id %in% samples_to_show) %>% 
  ggplot(aes(x = x, y = y_est, group = sample_id, color = as.factor(.chain))) + geom_line(alpha = 0.3) +
  geom_point(data = data.frame(x = data_gp$x, y = data_gp$y), aes(x = x, y = y), inherit.aes = FALSE)
```

And now a magic trick that turns this well-behaved model into a mess: we'll treat $x$, the locations where the GP is observed as unknown. Since $x \in (0,1)$, we can specify a Beta prior for the locations with varying precision. The modified Stan code follows.

```{r, results='hide', cache=TRUE}
stan_code_gp_mess <- "
data {
  int N;
  vector[N] y;
  real<lower=0> gp_length;
  real<lower=0> gp_scale;
  real<lower=0> sigma;
  vector<lower=0, upper=1>[N] x_prior_mean;
  real<lower=0> x_prior_tau;
}

transformed data {
  vector[N] mu = rep_vector(0, N);
}

parameters {
  real<lower=0, upper = 1> x[N];
  vector[N] y_est_raw;
}

transformed parameters {
  vector[N] y_est;
  // Using the latent variable GP coding form Stan manual, 
  // with the Cholesky decomposition
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, gp_scale, gp_length);
    for (n in 1:N) {
      K[n, n] = K[n, n] + 1e-12; //Ensure positive definite
    }
    L_K = cholesky_decompose(K);
    y_est = L_K * y_est_raw;
  }
}

model {
  y ~ normal(y_est, sigma);
  y_est_raw ~ normal(0, 1);
  x ~ beta(x_prior_mean * x_prior_tau,  (1 - x_prior_mean) * x_prior_tau);
}

"

model_gp_mess <- stan_model(model_code = stan_code_gp_mess)
```

Let's start with noninformative uniform prior on $x$.

```{r, cache=TRUE}
set.seed(42148744)

data_gp_mess_uniform <- data_gp
#This puts uniform prior on all x
data_gp_mess_uniform$x_prior_mean = rep(0.5, data_gp$N)
data_gp_mess_uniform$x_prior_tau = 2

fit_gp_mess <- sampling(model_gp_mess, data = data_gp_mess_uniform)
show_param_diags(fit_gp_mess) %>% head()

```

We can note that we got *both* divergences and max treedepth, meaning that the step size is sometimes too large and sometimes too small, also both `n_eff` and `Rhat` are atrocious. Let's inspect some traces:

```{r, warning=FALSE}
fit_for_bayesplot <- as.array(fit_gp_mess)
mcmc_trace(fit_for_bayesplot, pars = c("x[1]","y_est[1]", "lp__"), facet_args = list(ncol = 1)) + scale_color_discrete()
```

In contrast to the neural net example, here the chains do not stick to a single mode, instead, they slowly wander across a wide range of values. Further, we see that the log posterior more or less overlaps across the explored parts of the parameter space. How does that look like when we plot the posterior?

```{r, warning = FALSE}
plot_gp_mess <- function(fit_gp_mess) {
  samples_to_show <- sample(1:4000, 50)
  fit_gp_mess %>% 
    tidybayes::spread_samples(y_est[x_index], x[x_index]) %>% 
    mutate(sample_id = (.chain - 1) * 1000 + .iteration ) %>%
    filter(sample_id %in% samples_to_show) %>% 
    ggplot(aes(x = x, y = y_est, group = sample_id, color = as.factor(.chain))) + geom_line(alpha = 0.3) +
    geom_point(data = data.frame(x = data_gp$x, y = data_gp$y), aes(x = x, y = y), inherit.aes = FALSE)
}

plot_gp_mess(fit_gp_mess)
```

Not even close! We also see that the chains actually do sample different regions of the whole posterior, despite overlapping in marginal posteriors. Lets also take a look at some pairs plots:

```{r}
pairs(fit_gp_mess, pars = c("x[1]","y_est[1]", "lp__"))
```

Those pairs are not of much help, except that they once again show that there is no clean separation in log posterior (`lp__`) between the individual modes. Taken together this indicates that the posterior likely has many modes connected by thin, possibly curved ridges with only slightly smaller `lp__`. This is not surprising, since ordering of $x$ is only weakly constrained by the GP variance (when $x$ is close together, $y$ should be as well). In fact, we would expect the number of modes to be of order $N!$ (factorial of $N$).

A nice diagnostic trick is to set informative priors on $x$, centered on the true values. The width of the prior required to make the model identified tells us something about the severity of the issues. This is where the Beta prior, in particular it's parametrization via mean and precision ($\tau$) comes in handy. So, lets see if $\tau = 1000$, e.g. the amount of information contained in 1000 coin flips is enough. Note that we also have to init the chains around the true values, otherwise the sharp prior introduces sampling problems.

```{r, cache=TRUE}
set.seed(741284)
data_gp_mess_informed <- data_gp
data_gp_mess_informed$x_prior_mean = data_gp$x
data_gp_mess_informed$x_prior_tau = 1000

informed_init <- function(){
  list(x = data_gp$x)
}

fit_gp_mess_informed <- sampling(model_gp_mess, data = data_gp_mess_informed, init = informed_init)
show_param_diags(fit_gp_mess_informed) %>% head()

plot_gp_mess(fit_gp_mess_informed)

```

And it indeed is enough, the diagnostics look good, the posterior looks good, everything's peachy. But we needed a very narrow prior. And we could not get away with much less information, consider setting $\tau = 500$ (still a very narrow prior): 

```{r, cache=TRUE}
set.seed(32148422)
data_gp_mess_less_informed <- data_gp
data_gp_mess_less_informed$x_prior_mean = data_gp$x
data_gp_mess_less_informed$x_prior_tau = 500

fit_gp_mess_less_informed <- sampling(model_gp_mess, data = data_gp_mess_less_informed, init = informed_init)
show_param_diags(fit_gp_mess_less_informed) %>% head()


plot_gp_mess(fit_gp_mess_less_informed)

```

Even though the posterior looks more or less OK, we see that the chains have not mixed well (notably, chain 2 forms a slightly separate cluster) as also indicated by some of the diagnostics. So we can conclude that the model is screwed as it is not identified, unless we already now the values of $x$ quite precisely.

**Take home message**: Chains wandering slowly across large areas of posterior but with roughly the same `lp__` is a very bad sign. Putting narrow priors centered on true parameters is a neat trick to understand your model better.

# Conclusion

That's it - if you've made it to the bottom of this looooong post, you are great and thanks very much! I really hope that it will help you interpret your own models and help determine how to fix them or when to abandon a hopeless situation and get back to the drawing board. I also hope I have convinced you that identifiability depends not only on the model but also on the actual observed dataset.

Best of luck with modelling!

# Computing environment


```{r}
sessionInfo()
```

