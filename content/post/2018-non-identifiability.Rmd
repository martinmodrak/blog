---
title: "Identifying non-identifiability"
date: 2018-05-14
tags: ["R","Stan","Divergences"]
draft: true
---

*This is a second post in my series on taming divergences in Stan models, see the [first post in the series](/2018/02/19/taming-divergences-in-stan-models/) for a general introduction. *

**Standard caveat:** *I am not an expert on Stan, I consider myself just an advanced user who likes to explain things. Please point out any errors, things that contradict your experience or anything else you do not trust.*

# What is "non-identifiability"

In a strict sense, it means that there is not a unique local maximum of the posterior density - either because there are multiple separate maxima or because there is ridge/plateau where a set of points has the same posterior density.

On the Stan forums the term seems to be used in a bit broader sense and also covers cases where the maximum of the posterior density is in a region that is approximately flat. This often happens when the posterior is dominated by the prior and the likelihood provides little information about the parameters. If this is the case, it is sometimes said that the model is *weakly identified*. A weakly identified model may become non-identified in the strict sense if a prior is not specified for all parameters. This is just another reason to specify proper priors for everything.

# Scope

In this post I will show a few different types of issues that result from limited identifiability. I will also try to show how to spot these problems in various visualisations. Remember that instead of creating the plots in code as we do here, you can use [ShinyStan](http://mc-stan.org/users/interfaces/shinystan) to explore many visualisations interactively. 

We will start with some weakly identified non-linear regression models and move toward models that are hopelessly multimodal (have multiple local maxima of posterior density), including a small neural network and Gaussian process with a Berkson-style error. I will focus on models that don't have any obvious error (like ommitted prior), although such errors can lead to non-identifiability. A recurring theme in this post is that identifiability may depend on the actual data as well as the model, keep that in mind when modelling!

A frequent source of fitting issues due to non-identifiability are mixture models. There is an excellent
[case study for mixture models](http://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html) by Michael Betancourt and I have nothing to add to this topic, so those are not covered.

Well, let's get our hands dirty!

```{r setup, results='hide',message=FALSE,warning=FALSE}
library(tidyverse)
library(rstan)
library(bayesplot)
library(tidybayes)
library(knitr)
library(here)
library(rgl)
knit_hooks$set(webgl = hook_webgl)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```


# A weakly-identified model that (mostly) works

First, let's start with a model I thought would have problems but that ended up mostly OK. The model is a simple regression with a quadratic term:

$$
\begin{align}
\mu_i &= \beta_1 x_i + \beta_2 x_i^2 \\
y_i &\sim N(\mu_i, \sigma)
\end{align}
$$
and here is the Stan code:

```{r cache=TRUE, results='hide'} 
stan_code_linear <- " 
data { 
  int N; 
  vector[N] y; 
  vector[N] x; 
  real<lower=0> sigma; 
  real<lower=0> prior_width;
} 
 
parameters { 
  real beta[2]; 
} 
 
model { 
  y ~ normal(beta[1] * x + beta[2] * square(x), sigma); 
  sigma ~ normal(0,1); 
  beta ~ normal(0,prior_width); 
} 
" 
 
model_linear <- stan_model(model_code = stan_code_linear) 
``` 

When wide array of $x$ values is available, this model works without any trouble. But what happens when all $x_i \in  \{0,1\}$? In this case the likelihood cannot distinguish between the contribution of $\beta_1$ and $\beta_2$. Let's simulate some data and have a look at the [pairs](https://www.rdocumentation.org/packages/rstan/versions/2.17.3/topics/pairs.stanfit) plot:

```{r cache=TRUE} 
set.seed(20180512)

sigma = 1 
x = rep(c(0,1), times = 10)
data_linear <- list( 
  N = length(x), 
  x = x, 
  y = rnorm(length(x), x + x ^ 2, sigma),
  sigma = sigma,
  prior_width = 10
) 
 
fit_linear <- sampling(model_linear, data = data_linear) 
pairs(fit_linear, pars = "beta")
``` 

While the result is not very useful, the sampler worked well! We gained little information about each $beta$ individually (their range spans alomost all of the prior), but their sum is tightly constrained as witnessed by the strong negative correlation. But what if we increase `prior_width` to make the prior resemble a flat prior?

```{r cache=TRUE}
set.seed(21645465)

data_linear2 <- data_linear
data_linear2$prior_width = 100
fit_linear2 <- sampling(model_linear, data = data_linear2) 
pairs(fit_linear2, pars = "beta")
```

What happens is that the ridge in the posterior becomes too long and the sampler cannot traverse it efficiently, resulting in transitions that exceed the maximum treedepth. The root of the problem is that the sampler has to choose a step size that is shorter than the width of the ridge to not diverge when moving tangentially to the ridge direction. With such a small step size, the sampler cannot move across the length of the ridge in one iteration. Still, the results should be unbiased and if we manage to get a reasonable `n_eff` (which we do), there is no reason to worry.

While this particular model works well unless we set the prior too wide, such linear correlations in the pairs plot are a bad sign and you should try to avoid them as they can produce problems when interacting with other components of a larger model. It might make sense to reparametrize using the sum or ratio of the variables in question.

# A weakly-identified model leading to divergences

Let's move to a model where the non-identifiability actually wreaks havoc. Once again, the model works for some data, but breaks for others even if all model assumptions do hold. The model tries to determine parameters of a sigmoid function that is observed noisily:

$$
\begin{align}
y_{true} &= \frac{1}{1 + e^{-wx-b}} \\
y &\sim N(y_{true},\sigma)
\end{align}
$$
Here $w$ and $b$ are the only parameters of the model. The model is a bit artificial, but it is actually a component of larger gene expression models I work with. The corresponding Stan code is below:

```{r cache=TRUE, results = 'hide'}
stan_code_sigmoid <- "
data {
  int N;
  vector[N] y;
  vector[N] x;
  real<lower=0> prior_width;
  real<lower=0> sigma;
}

parameters {
  real w;
  real b;
}

model {
  vector[N] y_true = inv_logit(w * x + b);
  y ~ normal(y_true, sigma);
  w ~ normal(0,prior_width);
  b ~ normal(0,prior_width);
}
"

model_sigmoid <- stan_model(model_code = stan_code_sigmoid)
```

Now lets fit the model to simulated datasets with the exact same true parameter valuess $w = b = 1$, but different values of the independent variable $x$, in one case, $x$ is drawn from $N(0,2)$, in the other one, it is drawn from $N(0,5)$.

```{r cache=TRUE}
set.seed(214575878)

simulate_sigmoid <- function(x) {
  sigma = 0.1
  w = 1
  b = 1
  N = length(x)
  y_true = 1 / (1 + exp(-w*x-b))
  prior_width = 10
  
  list(
    N = N,
    x = x,
    y = rnorm(N, y_true, sigma),
    prior_width = prior_width,
    sigma = sigma
  )
}

data_sigmoid_ok <- simulate_sigmoid(rnorm(20, 0, 2))
fit_sigmoid_ok <- sampling(model_sigmoid, data = data_sigmoid_ok)
print(fit_sigmoid_ok)

data_sigmoid_divergent <-  simulate_sigmoid(rnorm(20, 5, 2)) # <- the mean of the distribution is altered!
fit_sigmoid_divergent <- sampling(model_sigmoid, data = data_sigmoid_divergent)
print(fit_sigmoid_divergent)
```

Now, for the first dataset, the model converges and recovers parameters correctly. For the second one there is a huge number of divergences and the parameters are largely uncertain and overestimated. So once again, the model is identifiable in principle, but the extreme data introduce problems. 

Let's try to visualise the posteriors - luckily we only have two parameters, so it is easy to see everything at once!

```{r}
pairs(fit_sigmoid_ok, pars = c("w","b"))
pairs(fit_sigmoid_divergent, pars = c("w","b"))
```

The issue is that for the second dataset, we sampled $x$ towards the tail of the sigmoid and almost all $y$ are thus close to 1, giving us little information about the parameters. However, the model strictly enforces that $w x + b > 0$.
This creates a large area of the posterior where the value of $w$ and $b$ does not matter much (requiring a large step size to traverse) and a thin sharp boundary around $wx + b \simeq 0$ where a smaller step size is required to traverse safely. Transitions crossing the boundary often diverge due to large step size, leading to overexploration of the flat area and bias.

This is, in my experience, one of the ways that weak identifiability may hurt sampling - the model maybe weakly identified only for a subset of the parameter space while there is another area where the likelihood has a huge contribution and this may require different step size for the sampler. 

The issue may be partially redeemed by reparametrization using $a = \mathrm{E}(wx + b), b = \mathrm{sd}(wx + b)$. You can then set priors on $a, b$ that avoid the tails of the sigmoid while being independent of $x$.

# A model with non-identified special case

We can make the above model even more problematic by introducing a parameter $k$ to generalize the sigmoid a bit more:

$$
\begin{align}
y_{true} &= \frac{k}{1 + e^{-wx-b}} \\
y &\sim N(y_{true},\sigma)
\end{align}
$$
giving us the following Stan code:

```{r cache=TRUE, results ='hide'}
stan_code_sigmoid2 <- "
data {
  int N;
  vector[N] y;
  vector[N] x;
  real<lower=0> prior_width;
  real<lower=0> sigma;
}

parameters {
  real k;
  real w;
  real b;
}

model {
  vector[N] y_true = k * inv_logit(w * x + b);
  y ~ normal(y_true, sigma);
  w ~ normal(0,prior_width);
  b ~ normal(0,prior_width);
}
"

model_sigmoid2 <- stan_model(model_code = stan_code_sigmoid2)
```
This time we will simulate two datasets with the same $x$ and $k = b = 1$, but one will have $w = 1$ while the other one will feature $w = 0$.

```{r cache=TRUE}
set.seed(98321456)

simulate_sigmoid2 <- function(x, w) {
  sigma = 0.1
  k = 1
  b = 1
  N = length(x)
  prior_width = 10
  y_true = k / (1 + exp(-w*x-b))
  
  list(
    N = N,
    x = x,
    y = rnorm(N, y_true, sigma),
    prior_width = prior_width,
    sigma = sigma
  )
}

data_sigmoid2_ok <- simulate_sigmoid2(rnorm(20, 0, 2), 1)
fit_sigmoid2_ok <- sampling(model_sigmoid2, data = data_sigmoid2_ok)
print(fit_sigmoid2_ok)

data_sigmoid2_divergent <-  simulate_sigmoid2(rnorm(20, 0, 2), 0)
fit_sigmoid2_divergent <- sampling(model_sigmoid2, data = data_sigmoid2_divergent)
print(fit_sigmoid2_divergent)
```

As has become the habit in this post, the model diverges for the second dataset. Note also that the parameter estimates for $k$ and $b$ are way off. (remember: even a few divergent transitions indicate sampling problems). Let's have a look at the pair plots:

```{r}
pairs(fit_sigmoid2_ok, pars = "energy__", include = FALSE)
pairs(fit_sigmoid2_divergent, pars = "energy__", include = FALSE)
```
While the interaction of $w$ and $b$ looks fishy, it is hard to understand what exactly is going on. The clue is in looking at the interaction of $w$, $k$ and the log posterior (`lp__`) - you can play with the 3D plot with your mouse. Note that ShinyStan provides similar 3D plots under `Explore -> Trivariate`.

```{r, webgl=TRUE}
samples <- rstan::extract(fit_sigmoid2_divergent)
open3d()
rgl::plot3d(samples$w, samples$k, samples$lp__, xlab = "w", ylab = "k", zlab = "lp__")
```

The thing to notice here is that for $w = 0$, there is a thin ridge in the posterior with almost maximal `lp__` for a wide range of values for $k$. This is because for $w = 0$, the posterior ceases to depend on $x$ and $k$ and $b$ become very closely tied, but almost any value of $k$ that is admitted by the prior becomes feasible with suitable value of $b$. The ridge is so thin, that the sampler almost completely misses it and it is barely visible, but for the true posterior, it should contribute a non-trivial amount of mass. Once again, the sampler adapts its step size to the wide distribution for $w \neq 0$ which leads to divergences when traversing the ridge at $w = 0$.

When you fit your model interactively, the best solution is to spot that your actual data are a special case and simplify your model. These kind of issues however become more of a worry when you need to automatically fit the model to a large number of datasets or refit it periodically as new data become available, providing that the data sometimes represent a special case and sometimes require the full model. The best solution I have found so far is to try fitting a simplified model first (here something like $x \sim N(a,\sigma))$). If the simplified model fits well and/or the full model diverges, the results of the simplified model are preferred. If you know of a better solution, please let me know - it is directly relevant to my work on [models of gene regulation](/post/2018-04-18-stancon/stancon.html)! Here you can also find more discussion of the reparametrizations I use to make this kind of models converge.

# Neural network: When ordering is not enough

At this point it would make sense to mention mixture models, but as those are covered by the aforementioned [case study](http://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html), we'll go straight ahead to the desperate wilderness of models too broken to fix. And neural networks are the prime attraction in this godforsaken land.

We don't need to go fancy. Let's have a feedforward neural net with two inputs, two hidden layers of two neurons each and a single output neuron.

![](/post/2018-non-identifiability_files/nn.png)

We will use the standard logistic sigmoid activation function and treat the problem as a binary classification. To make things simpler and because we saw that sigmoid be non-dentifiable by  itself, we ignore all the bias parameters. 
Below is a Stan model - optimized for readability, not brevity or generalizability. Since it seems there might be some symmetries, and we learned our lesson from mixture models, we'll try at least to force the weights for the output neuron to be ordered.

```{r, results='hide', cache=TRUE}
stan_code_neural <- "


data {
  int N;
  matrix[N,2] x;
  int<lower=0, upper=1> y[N];
  real prior_width;
}

parameters {
  matrix[2,2] weights1;
  matrix[2,2] weights2;
  ordered[2] weights_out;
}

model {
  matrix[N,2] input1 = x * weights1;
  matrix[N,2] output1 = inv_logit(input1);

  matrix[N,2] input2 = output1 * weights2;
  matrix[N,2] output2 = inv_logit(input2);

  vector[N] input_out = output2 * weights_out;
  vector[N] output_out = inv_logit(input_out);
  y ~ bernoulli(output_out);

  to_vector(weights1) ~ normal(0, prior_width);
  to_vector(weights2) ~ normal(0, prior_width);
  weights_out ~ normal(0, prior_width);
}
" 

model_neural <- stan_model(model_code = stan_code_neural)
```
And in the spirit of the best traditions of the field of machine learning, we'll try to teach XOR to the neural network. It does not go well.

```{r, cache=TRUE}
set.seed(1324578)
sigma <- 0.1
N <- 200

x <- array(as.integer(rbernoulli(N * 2)), c(N,2))
y <- xor(x[,1], x[,2])

data_neural <- list(N = N, x = x, y = y, sigma = sigma, prior_width = 5)
fit_neural <- sampling(model_neural, data = data_neural, chains = 8)
print(fit_neural)
```
```{r, warning = FALSE}
fit_for_bayesplot <- as.array(fit_neural)
mcmc_trace(fit_for_bayesplot, pars = c("weights2[1,2]","weights1[2,2]"), alpha = 0.1, facet_args = list(ncol = 1)) + scale_color_discrete()
```

And why are there divergences? Looking at the pairs plot provides some hints:

```{r}
pairs(fit_neural, pars = c("weights2[1,2]","lp__"))
```

My best guess is that the divergences arise because the sampler adapts to the wider mode and then has trouble exploring the two narrower modes, but this is just guesswork. Also note that all the modes have roughly the sampe posterior density, meaning that they truly represent separate, equally correct solutions.


There is not really much that can be done to make such models work. The most obvious issues come from symmetries of the network structure, providing multiple modes where the neurons are relabelled, but the network is the same. However, these are just the start and further issues just keep on coming - see for example [the forum thread on Bayesian neural networks](http://discourse.mc-stan.org/t/why-are-bayesian-neural-networks-multi-modal/3285/3) for more details.


# A hopelessly non-identified GP model

There is still a way to move non-identifiability to the next level.
GP with observation times as variables.


```{r, results='hide', cache=TRUE}
stan_code_gp <- "
data {
  int N;
  real x[N];
  vector[N] y;
  real<lower=0> gp_length;
  real<lower=0> gp_scale;
  real<lower=0> sigma;
}

transformed data {
  vector[N] mu = rep_vector(0, N);
}

parameters {
  vector[N] y_est_raw;
}

transformed parameters {
  vector[N] y_est;
  // Using the latent variable GP coding form Stan manual, 
  // with the Cholesky decomposition
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, gp_scale, gp_length);
    for (n in 1:N) {
      K[n, n] = K[n, n] + 1e-12; //Ensure positive definite
    }
    L_K = cholesky_decompose(K);
    y_est = L_K * y_est_raw;
  }
}

model {
  y ~ normal(y_est, sigma);
  y_est_raw ~ normal(0, 1);
}

"

model_gp <- stan_model(model_code = stan_code_gp)
```
```{r, cache=TRUE}
set.seed(25748422)
simulate_gp <- function(x) {
  N <- length(x)
  gp_length <- 0.3
  gp_scale <- 1
  sigma <- 0.1
  cov_m <- matrix(0, nrow <- N, ncol <- N)
  for(i in 1:N) {
    for(j in i:N) {
      cov_m[i,j] <- gp_scale ^ 2 * exp(-0.5 * (1 / gp_length ^ 2) * (x[i] - x[j])^2)
      cov_m[j,i] <- cov_m[i,j]
    }
  }
  chol_cov_m <- chol(cov_m)
  y <- chol_cov_m %*% rnorm(N, 0, 1)
  
  list(N = N, x = x, y = array(y, N), gp_length = gp_length, gp_scale = gp_scale, sigma = sigma)
}

data_gp <- simulate_gp(x = seq(0.01,0.99, length.out = 10))
fit_gp <- sampling(model_gp, data = data_gp)
summary(fit_gp)$summary %>% head()
```
```{r}
samples_to_show <- sample(1:4000, 50)
fit_gp %>% 
  tidybayes::spread_samples(y_est[x_index]) %>% 
  inner_join(data.frame(x_index = 1:data_gp$N, x = data_gp$x), by = c("x_index" = "x_index")) %>% 
  mutate(sample_id = (.chain - 1) * 1000 + .iteration ) %>%
  filter(sample_id %in% samples_to_show) %>% 
  ggplot(aes(x = x, y = y_est, group = sample_id, color = .chain)) + geom_line(alpha = 0.1, color = "blue") +
  geom_point(data = data.frame(x = data_gp$x, y = data_gp$y), aes(x = x, y = y), inherit.aes = FALSE)
```

```{r, results='hide', cache=TRUE}
stan_code_gp_mess <- "
data {
  int N;
  vector[N] y;
  real<lower=0> gp_length;
  real<lower=0> gp_scale;
  real<lower=0> sigma;
  vector<lower=0, upper=1>[N] x_prior_mean;
  real<lower=0> x_prior_tau;
}

transformed data {
  vector[N] mu = rep_vector(0, N);
}

parameters {
  real<lower=0, upper = 1> x[N];
  vector[N] y_est_raw;
}

transformed parameters {
  vector[N] y_est;
  // Using the latent variable GP coding form Stan manual, 
  // with the Cholesky decomposition
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, gp_scale, gp_length);
    for (n in 1:N) {
      K[n, n] = K[n, n] + 1e-12; //Ensure positive definite
    }
    L_K = cholesky_decompose(K);
    y_est = L_K * y_est_raw;
  }
}

model {
  y ~ normal(y_est, sigma);
  y_est_raw ~ normal(0, 1);
  x ~ beta(x_prior_mean * x_prior_tau,  (1 - x_prior_mean) * x_prior_tau);
}

"

model_gp_mess <- stan_model(model_code = stan_code_gp_mess)
```


```{r, cache=TRUE}
set.seed(42148744)

data_gp_mess_uniform <- data_gp
#This puts uniform prior on all x
data_gp_mess_uniform$x_prior_mean = rep(0.5, data_gp$N)
data_gp_mess_uniform$x_prior_tau = 2

fit_gp_mess <- sampling(model_gp_mess, data = data_gp_mess_uniform)
summary(fit_gp_mess)$summary %>% head()
```

```{r, warning = FALSE}
fit_for_bayesplot <- as.array(fit_gp_mess)
mcmc_trace(fit_for_bayesplot, pars = c("x[1]","y_est[1]"), alpha = 0.1, facet_args = list(ncol = 1)) + scale_color_discrete()


plot_gp_mess <- function(fit_gp_mess) {
  samples_to_show <- sample(1:4000, 50)
  fit_gp_mess %>% 
    tidybayes::spread_samples(y_est[x_index], x[x_index]) %>% 
    mutate(sample_id = (.chain - 1) * 1000 + .iteration ) %>%
    filter(sample_id %in% samples_to_show) %>% 
    ggplot(aes(x = x, y = y_est, group = sample_id, color = as.factor(.chain))) + geom_line(alpha = 0.3) +
    geom_point(data = data.frame(x = data_gp$x, y = data_gp$y), aes(x = x, y = y), inherit.aes = FALSE)
}

plot_gp_mess(fit_gp_mess)
```

```{r, cache=TRUE}
set.seed(741284)
data_gp_mess_informed <- data_gp
data_gp_mess_informed$x_prior_mean = data_gp$x
data_gp_mess_informed$x_prior_tau = 1000

informed_init <- function(){
  list(x = data_gp$x)
}

fit_gp_mess_informed <- sampling(model_gp_mess, data = data_gp_mess_informed, init = informed_init)
summary(fit_gp_mess_informed)$summary %>% head()

plot_gp_mess(fit_gp_mess_informed)

```


```{r, cache=TRUE}
set.seed(32148422)
data_gp_mess_less_informed <- data_gp
data_gp_mess_less_informed$x_prior_mean = data_gp$x
data_gp_mess_less_informed$x_prior_tau = 500

fit_gp_mess_less_informed <- sampling(model_gp_mess, data = data_gp_mess_less_informed, init = informed_init)
summary(fit_gp_mess_less_informed)$summary %>% head()


plot_gp_mess(fit_gp_mess_less_informed)

```

# Computing environment

```{r}
sessionInfo()
```

